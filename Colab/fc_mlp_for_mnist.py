# -*- coding: utf-8 -*-
"""FC_MLP for MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1POzi3c_0hf2Z5gUckGTi614nmW1d1dYG

an MNIST digit classifier built with a fully-connected neural network in TensorFlow and Keras.

Ready Tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt

# %tensorflow_version 2.x
import tensorflow as tf

# Load the MNIST dataset.
mnist = tf.keras.datasets.mnist

train_data, test_data = mnist.load_data()

# Divide the data into features(images) and labels.
train_images, train_labels = train_data
test_images, test_labels = test_data

# X_train = train_images[0:50000]
# X_validation = train_images[50000:60000]
# y_validation = train_labels[7000:10000]
# y_train = train_labels[0:7000]

# Reshape and normalize the images.
X_train = train_images.reshape((60000, 784))
X_train = X_train.astype('float32') / 255
X_test = test_images.reshape((10000, 784))
X_test = X_test.astype('float32') / 255

# Reshape the labels and encode them categorically.
y_train = tf.keras.utils.to_categorical(train_labels)
y_test = tf.keras.utils.to_categorical(test_labels)

# Show the shapes of the data.
print("Training Images:", X_train.shape)
print("Testing Images:", X_test.shape)
print("Training Labels:", y_train.shape)
print("Test Labels:", y_test.shape)

# Show a sample MNIST digit.
plt.imshow(train_images[748])
plt.show()

# Define the sequential model.
model = tf.keras.models.Sequential([
        # input layer. input data with size 28*28 and output size 256
        # 256 means you set up your modul with 256 NN in this layer.
        tf.keras.layers.Dense(256, input_shape=(28*28,), activation='relu'),

        # hidden layer. input data with size 256, which were same to output of input layer.
        # output size 256, we set up 256 NN again in this hidden layer.
        # no need to give input size here because keras already know.
        tf.keras.layers.Dense(256, activation='relu'),

        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.BatchNormalization(),  # Add batch normalization layer

        # output layer. the number of output should be your number of classification
        tf.keras.layers.Dense(10, activation='softmax')
    ])

# # Add two fully-connected layers to the network.
# model.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
# model.add(tf.keras.layers.Dense(10, activation='softmax'))

# Show the model.
model.summary()

# Compile the model.
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define a callback to save the model when validation loss improves.
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='mnist_model.h5',
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

# Define the parameters.
num_epochs = 30
batch_size = 256

# Train the model.
history = model.fit(X_train, y_train,
                    epochs=num_epochs,
                    batch_size=batch_size,
                    # validation_data=(X_validation, y_validation),
                    validation_split=0.2)

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print('Test Accuracy:', test_accuracy)
print('Test Loss:', test_loss)

# Save the metrics.
metrics = history.history

# Save the loss values.
training_loss_list = metrics['loss']
test_loss_list = metrics['val_loss']

# Plot the training and test loss.
x = np.arange(0, num_epochs, 1)
plt.title('Training and Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(x, training_loss_list, label='Training Loss')
plt.plot(x, test_loss_list, label='Test Loss')
plt.legend()
plt.show()

train_accuracy_list = metrics['accuracy']
test_accuracy_list = metrics['val_accuracy']

plt.title('Training and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(x, train_accuracy_list, label='Training Accuracy')
plt.plot(x, test_accuracy_list, label='Test Accuracy')
plt.legend()
plt.show()

# Make predictions with the trained model.
predictions = model.predict(X_test)

# Choose an index.
index = 5

# Show an image from the test set.
plt.imshow(test_images[index])
plt.show()

print("Prediction:", np.argmax(predictions[index]))